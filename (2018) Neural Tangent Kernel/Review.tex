\documentclass[10pt]{article}

\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{latexsym}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{tikz-cd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{parskip}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{bm}

\newenvironment{solution}{\textit{Solution.}}{\hfill$\square$}

\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}

\newcommand{\CC}{\mathcal{C}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\OO}{\mathcal{O}}

\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\paran}[1]{{( #1 )}}
\newcommand{\pin}{{p^{in}}}
\newcommand{\din}{\partial^{in}}
\newcommand{\op}{{op}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{Paper Review: Neural Tangent Kernel}
\author{Beomsu Kim\footnote{Department of Mathematical Sciences, KAIST. Email \texttt{beomsu.kim@kaist.ac.kr}}}
\date{\today}

\begin{document}

\maketitle

\textbf{Paper Information.}

\begin{itemize}
\item Arthur Jacot, Franck Gabriel, and Cl\'{e}ment Hongler. Neural Tangent Kernel. \newline In \underline{Neural Information Processing Systems}, 2018.
\end{itemize}

\section{Introduction}

\section{Neural Networks}

\begin{itemize}
\item We consider fully-connected ANNs with layers numbered from $0$ (input) to $L$ (output).
\item $n_l$ : number of neurons in layer $l$.
\item $\sigma : \RR \rightarrow \RR$ : Lipschitz, twice differentiable nonlinearity function, with bounded second derivative.
\item $\theta$ : weights $W^\paran{l} \in \RR^{n_l \times n_{l + 1}}$ and bias vectors $b^\paran{l} \in \RR^{n_l + 1}$. Initialized as i.i.d. Gaussians $\NN(0,1)$.
\item $P = \sum_{l = 0}^{L - 1} (n_l + 1) n_{l + 1}$ : number of parameters.
\item $\FF = \{f : \RR^{n_0} \rightarrow \RR^{n_L}\}$ : space of functions.
\item $F^\paran{L} : \RR^P \rightarrow \FF$ : ANN realization function, mapping parameters to the functions $f_\theta \in \FF$.
\item $\pin = \sum_{i = 1}^N \delta_{x_i}$ : input distribution.
\item $\inner{f}{g}_{\pin} = \EE_{x \sim \pin} [f(x)^\top g(x)]$ : bilinear form defined on $\pin$.
\item $\| f \|_{\pin} = \inner{f}{f}_{\pin}^{1/2}$ : seminorm defined on $\pin$.
\item Define the functions
\begin{align*}
\alpha^\paran{0}(x;\theta) &= x, \\
\tilde{\alpha}^\paran{l + 1}(x;\theta) &= \frac{1}{\sqrt{n_l}} W^\paran{l} \alpha^\paran{l}(x;\theta) + \beta b^\paran{l}, \\
\alpha^\paran{l}(x;\theta) &= \sigma(\tilde{\alpha}^\paran{l}(x;\theta)), \\
f_\theta(x) &= \tilde{\alpha}^\paran{L}(x;\theta).
\end{align*}
\end{itemize}

\newpage

\section{Kernel Gradient}

\begin{itemize}
\item $C : \FF \rightarrow \RR$ : functional cost.
\item $K : \RR^{n_0 \times n_0} \rightarrow \RR^{n_L \times n_L}$ : multi-dimensional kernel which satisfies $K(x,x') = K(x',x)^\top$.
\item $\inner{f}{g}_K = \EE_{x,x' \sim \pin} [f(x)^\top K(x,x') g(x')]$ : inner product w.r.t. kernel $K$.
\item The kernel $K$ is positive definite w.r.t. $\|\cdot\|_{\pin}$ if $\|f\|_{\pin} > 0 \implies \|f\|_K > 0$.
\item $\FF^* = \{ \mu = \inner{d}{\cdot}_{\pin} : d \in \FF \}$ : the dual space of $\FF$.
\item $\Phi_K : \FF^* \rightarrow \FF$ is defined such that
\begin{align*}
\Phi_K : \inner{d}{\cdot}_{\pin} \mapsto \frac{1}{N} \sum_{i = 1}^N K(\cdot,x_i) d(x_i).
\end{align*}
$\Phi_K$ can be interpreted as a map which interpolates $d$ using the kernel $K$.
\item $\din_f C|_{f_0} = \inner{d|_{f_0}}{\cdot}_{\pin}$ : functional derivative of $C$ at a point $f_0 \in \FF$.
\item $\nabla_K C|_{f_0} = \Phi_K(\din_f C|_{f_0})$ : kernel gradient.
\item In contrast to $\din_f C$ which is only defined on the dataset, the kernel gradient generalizes to values $x$ outside the dataset thanks to the kernel $K$.
\item A time-dependent function $f(t)$ follows the kernel gradient descent w.r.t. $K$ if it satisfies
\begin{align*}
\partial_t f(t) = -\nabla_K C|_{f(t)} = - \Phi_K(\din_f C|_{f(t)}) = -\frac{1}{N} \sum_{i = 1}^N K(\cdot,x_i) d|_{f(t)}(x_i).
\end{align*}
\item During kernel gradient descent, the cost $C(f(t))$ evolves as
\begin{align*}
\partial_t C|_{f(t)} = \partial_t C(f(t)) &= \din_f C|_{f(t)}(\partial_t f(t)) \\
&= \inner{d|_{f(t)}}{\partial_t f(t)}_{\pin} \\
&= \inner{d|_{f(t)}}{-\frac{1}{N} \sum_{i = 1}^N K(\cdot,x_i) d|_{f(t)}(x_i)}_\pin \\
&= \frac{1}{N} \sum_{j = 1}^N d|_{f(t)}(x_j)^\top \left( -\frac{1}{N} \sum_{i = 1}^N K(x_j,x_i) d|_{f(t)}(x_i) \right) \\
&= -\frac{1}{N^2} \sum_{j = 1}^N \sum_{i = 1}^N d|_{f(t)}(x_j)^\top K(x_j,x_i) d|_{f(t)}(x_i) \\
&= - \EE_{x,x' \sim \pin} [d|_{f(t)}(x)^\top K(x,x') d|_{f(t)}(x')] \\
&= - \| d|_{f(t)} \|_K^2.
\end{align*}
Convergence to a critical point of $C$ is hence guaranteed if the kernel $K$ is positive definite with respect to $\|\cdot\|_{\pin}$ : the cost is then strictly decreasing except at points such that $\|d|_{f(t)}\|_{\pin} = 0$. If the cost is convex and bounded from below, the function $f(t)$ therefore converges to a global minimum as $t \rightarrow \infty$.
\item For our setup, which is that of a finite dataset $x_1, \ldots, x_n \in \RR^{n_0}$, the cost function $C$ only depends on the values of $f \in \FF$ at the data points. Hence, the global minimum may not be unique. Specifically, any function which minimizes the cost functional on the data points is a global minimum. For instance, see Figure 2 in the paper.
\end{itemize}

\newpage

\subsection{Random Functions Approximation}

\begin{itemize}
\item A kernel $K$ can be approximated by a choice of $P$ random functions $f^\paran{p}$ sampled independently from any distribution on $\FF$ whose (non-centered) covariance is given by the kernel $K$:
\begin{align*}
\EE [f^\paran{p}(x) f^\paran{p}(x')^\top] = K(x,x')
\end{align*}
or equivalently,
\begin{align*}
\EE [f^\paran{p}_k(x) f^\paran{p}_{k'}(x')] = K_{kk'}(x,x').
\end{align*}
\item These functions define a random linear parametrization
\begin{align*}
F^{lin} : \RR^P \rightarrow \FF : \theta \mapsto f^{lin}_\theta = \frac{1}{\sqrt{P}} \sum_{p = 1}^P \theta_p f^\paran{p}.
\end{align*}
\item The partial derivatives of the parametrization are given by ($e_p$ is the $p$-th standard basis vector)
\begin{align*}
\partial_{\theta_p} F^{lin}(\theta) = \lim_{h \rightarrow 0} \frac{F^{lin}(\theta + h e_p) - F^{lin}(\theta)}{h} = \frac{1}{\sqrt{P}} f^\paran{p}.
\end{align*}
\item Optimizing the cost $C \circ F^{lin}$ through gradient descent, the parameters follow the ODE
\begin{align*}
\partial_t \theta_p(t) = -\partial_{\theta_p} (C \circ F^{lin})(\theta(t)) &= -\partial_{\theta_p} C(f^{lin}_{\theta(t)}) \\
&= - \din_f C|_{f^{lin}_{\theta(t)}}(\partial_{\theta_p} f^{lin}_{\theta(t)}) \\
&= -\frac{1}{\sqrt{P}} \din_f C|_{f^{lin}_{\theta(t)}}(f^\paran{p}) = -\frac{1}{\sqrt{P}} \inner{d|_{f^{lin}_{\theta(t)}}}{f^\paran{p}}_\pin.
\end{align*}
The first equality holds since we are performing gradient descent, i.e., the instantaneous change of $\theta_p$ at time $t$ must equal the gradient of $\theta_p$ w.r.t. the cost at time $t$.
\item As a result, the function $f^{lin}_{\theta(t)}$ evolves according to
\begin{align*}
\partial_t f^{lin}_{\theta(t)} = \partial_t \left( \frac{1}{\sqrt{P}} \sum_{p = 1}^P \theta_p(t) f^\paran{p} \right) &= \frac{1}{\sqrt{P}} \sum_{p = 1}^P \partial_t \theta_p(t) f^\paran{p} \\
&= -\frac{1}{P} \sum_{p = 1}^P \inner{d|_{f^{lin}_{\theta(t)}}}{f^\paran{p}}_{\pin} f^\paran{p} \\
&= -\frac{1}{P} \sum_{p = 1}^P \frac{1}{N} \sum_{i = 1}^N d|_{f^{lin}_{\theta(t)}}(x_i)^\top f^\paran{p}(x_i) f^\paran{p}(\cdot) \\
&= -\frac{1}{P} \sum_{p = 1}^P \frac{1}{N} \sum_{i = 1}^N (f^\paran{p} \otimes f^\paran{p})(\cdot,x_i) d|_{f^{lin}_{\theta(t)}}(x_i) \\
&= -\frac{1}{N} \sum_{i = 1}^N \left( \frac{1}{P} \sum_{p = 1}^P f^\paran{p} \otimes f^\paran{p} \right)(\cdot,x_i) d|_{f^{lin}_{\theta(t)}}(x_i) \\
&= -\Phi_{\tilde{K}}(\din_f C|_{f^{lin}_{\theta(t)}}) \\
&= -\nabla_{\tilde{K}} C|_{f^{lin}_{\theta(t)}}
\end{align*}
where
\begin{align*}
\tilde{K} = \sum_{p = 1}^P \partial_{\theta_p} F^{lin}(\theta) \otimes \partial_{\theta_p} F^{lin}(\theta) = \frac{1}{P} \sum_{p = 1}^P f^\paran{p} \otimes f^\paran{p}.
\end{align*}
\item This is a random $n_L$-dimensional kernel with values
\begin{align*}
\tilde{K}_{ii'}(x,x') = \frac{1}{P} \sum_{p = 1}^P f^\paran{p}_i(x) f^\paran{p}_{i'}(x').
\end{align*}
\item Performing gradient descent on the cost $C \circ F^{lin}$ is therefore equivalent to performing kernel gradient descent with the tangent kernel $\tilde{K}$ in the function space.
\item With $P \rightarrow \infty$, by the law of large numbers, the random kernel $\tilde{K}$ tends to the fixed kernel $K$.
\begin{align*}
\lim_{P \rightarrow \infty} \tilde{K}_{ii'}(x,x') = \lim_{P \rightarrow \infty} \frac{1}{P} \sum_{p = 1}^P f^\paran{p}_i(x) f^\paran{p}_{i'}(x') = \EE[f^\paran{p}_i(x) f^\paran{p}_{i'}(x')] = K_{ii'}(x,x').
\end{align*}
Hence, this method approximates kernel gradient descent with respect to the limiting kernel $K$.
\end{itemize}

\section{Neural Tangent Kernel}

\begin{itemize}
\item During training, the network function $f_\theta$ evolves along the negative kernel gradient
\begin{align*}
\partial_t f_{\theta(t)} = -\nabla_{\Theta^\paran{L}} C|_{f_{\theta(t)}}
\end{align*}
with respect to the neural tangent kernel (NTK)
\begin{align*}
\Theta^\paran{L}(\theta) = \sum_{p = 1}^P \partial_{\theta_p} F^\paran{L}(\theta) \otimes \partial_{\theta_p} F^\paran{L}(\theta).
\end{align*}
This can be derived by following the steps in Section 3.1 with $F^\paran{L}$ in place of $F^{lin}$.
\item However, in contrast to $F^{lin}$, the realization function $F^\paran{L}$ of ANNs is not linear.
\item As a consequence, the derivatives $\partial_{\theta_p} F^\paran{L}(\theta)$ and the NTK depend on the parameters $\theta$.
\end{itemize}

\subsection{Initialization}

\begin{itemize}
\item The first key result is that in the limit $n_1, \ldots, n_{L - 1} \rightarrow \infty$, the NTK converges in probability to a deterministic limiting kernel.
\end{itemize}

\subsection{Training}

\begin{itemize}
\item The second key result is that the NTK stays asymptotically constant during training.
\item In general, the parameters can be updated according to a training direction $d_t \in \FF$.
\begin{align*}
\partial_t \theta_p(t) = \inner{\partial_{\theta_p} F^\paran{L}(\theta(t))}{d_t}_\pin
\end{align*}
\item In the case of gradient descent,
\begin{align*}
\partial_t \theta_p(t) = -\partial_{\theta_p}(C \circ F^\paran{L})(\theta(t)) &= -\partial_{\theta_p} C(f_{\theta(t)}) \\
&= -\din_f C|_{f_{\theta(t)}}(\partial_{\theta_p} f_{\theta(t)}) \\
&=  \inner{\partial_{\theta_p} f_{\theta(t)}}{-d|_{f_{\theta(t)}}}_\pin \\
&= \inner{\partial_{\theta_p} F^\paran{L}(\theta(t))}{-d|_{f_{\theta(t)}}}_\pin
\end{align*}
and so
\begin{align*}
d_t = -d|_{f_{\theta(t)}}.
\end{align*}
\item The limiting NTK is positive definite if the span of the derivatives $\partial_{\theta_p} F^\paran{L}$ becomes dense in $\FF$ w.r.t. the $\pin$ norm as the width grows to infinity. See the Proposition below.
\end{itemize}

\textbf{Proposition.} \textit{Let $\FF$ be an infinite-dimensional vector space equipped with an inner product. Let $\{f_n\}_{n = 1}^\infty$ be a set of vectors in $\FF$ such that its span is dense in $\FF$. That is, finite linear combinations of elements in $\{f_n\}_{n = 1}^\infty$ is are dense in $\FF$. Define (assuming it exists)
\begin{align*}
\Theta = \sum_{n = 1}^\infty f_n \otimes f_n.
\end{align*}
Then $\|g\| > 0$ implies that $\|g\|_\Theta > 0$, i.e., $\Theta$ is positive definite.}

\begin{proof}
Without loss of generality, we assume $\{f_n\}_{n = 1}^\infty$ is linearly independent. By the Gram-Schmidt process, we may also assume $\{f_n\}_{n = 1}^\infty$ is orthonormal. Suppose there is $g \in \FF$ such that $\|g\|_\Theta = 0$, i.e.,
\begin{align*}
0 = \|g\|_\Theta^2 = \sum_{n = 1}^\infty g^\top (f_n \otimes f_n) g = \sum_{n = 1}^\infty \inner{f_n}{g}^2.
\end{align*}
(We need to check whether we can interchange the inner product with $g$ and the infinite sum, but I'm being lazy here.) This implies that
\begin{align*}
\inner{f_n}{g} = 0 \quad \forall n.
\end{align*}
Since $\{f_n\}_{n = 1}^\infty$ is an orthonormal basis of $\FF$, $g = 0$ and so $\|g\| = 0$. (See, for instance, Theorem 4.2.3 of \textit{Real Analysis} by Stein and Shakarchi.)
\end{proof}

\section{Least-Squares Regression}

\begin{itemize}
\item Given a goal function $f^*$ and input distribution $\pin$, the least-squares regression cost is
\begin{align*}
C(f) = \frac{1}{2} \|f - f^*\|^2_\pin = \frac{1}{2} \EE_{x \sim \pin} [ \|f(x) - f^*(x)\|^2 ].
\end{align*}
\item We are interested in the behavior of a function $f_t$ during kernel gradient descent with a kernel $K$.
\begin{align*}
\partial_t f_t = \Phi_K(\inner{f^* - f}{\cdot}_\pin)
\end{align*}
If we define the map $\Pi : f \mapsto \Phi_K(\inner{f}{\cdot}_\pin)$, this differential equation is equivalent to
\begin{align*}
\partial_t f_t = \Pi(f - f^*).
\end{align*}
Since $\Pi$ is a linear operator on $\FF$, the solution of this differential equation can be expressed as
\begin{align*}
f_t = f^* + e^{-t\Pi}(f_0 - f^*)
\end{align*}
where
\begin{align*}
e^{-t\Pi} = \sum_{k = 0}^\infty \frac{(-t)^k}{k!} \Pi^k.
\end{align*}
For instance, see \url{https://en.wikipedia.org/wiki/Matrix\_differential\_equation}.
\item If $\Pi$ can be diagonalized by eigenfunctions $f^\paran{i}$ with eigenvalues $\lambda_i$, the exponential $e^{-t\Pi}$ has the same eigenfunctions with eigenvalues $e^{-t \lambda_i}$. Specifically,
\begin{align*}
e^{-t\Pi}(f^\paran{i}) = \sum_{k = 0}^\infty \frac{(-t)^k}{k!} \Pi^k(f^\paran{i}) = \sum_{k = 0}^\infty \frac{(-t \lambda_i)^k}{k!} f^\paran{i} = e^{-t\lambda_i} f^\paran{i}.
\end{align*}
\item For a finite dataset $x_1, \ldots, x_N$ of size $N$, the map $\Pi$ takes the form
\begin{align*}
\Pi(f)_k(x) = \frac{1}{N} \sum_{i = 1}^N \sum_{k' = 1}^{n_L} f_{k'}(x_i) K_{kk'}(x_i,x).
\end{align*}
This is from the definition of the maps $\Pi$ and $\Phi_K$.
\item The map $\Pi$ has at most $Nn_L$ positive eigenfunctions since each summand $f_{k'}(x_i)K_{kk'}(x_i,x)$ can contribute at most one eigenfunction. The eigenfunctions are the kernel principal components $f^\paran{1}, \ldots, f^\paran{Nn_L}$ of the data with respect to the kernel $K$. To understand what this means, consider the case $n_L = 1$ so
\begin{align*}
\Pi(f)(x) = \frac{1}{N} \sum_{i = 1}^N f(x_i) K(x_i,x).
\end{align*}
Each eigenfunction $f^\paran{n}$ must satisfy
\begin{align*}
\lambda_n f^\paran{n}(x_j) = \Pi(f^\paran{n})(x_j) = \frac{1}{N} \sum_{i = 1}^N f^\paran{n}(x_i) K(x_i,x_j)
\end{align*}
or in matrix form,
\begin{align*}
N \lambda_n (f^\paran{n}(x_j))_j = (K(x_i,x_j))_{i,j} (f^\paran{n}(x_j))_j.
\end{align*}
This coincides with Equation (8) in \url{https://people.eecs.berkeley.edu/~wainwrig/stat241b/scholkopf_kernel.pdf}. The corresponding eigenvalues $\lambda_i$ is the variance captured by the component.
\item Decomposing the difference
\begin{align*}
f^* - f_0 = \Delta^0_f + \Delta^1_f + \cdots + \Delta^{Nn_L}_f
\end{align*}
along the eigenspaces of $\Pi$, the trajectory of the function $f_t$ reads
\begin{align*}
f_t = f^* + \Delta^0_f + \sum_{i = 1}^{Nn_L} e^{-t \lambda_i} \Delta^i_f,
\end{align*}
where $\Delta^0_f$ is in the kernel (null-space) of $\Pi$ and $\Delta^i_f \propto f^\paran{i}$.
\item Note that by the linearity of the map $e^{-t\Pi}$, if $f_0$ is initialized with a Gaussian distribution (as in the case for ANNs in the infinite-width limit), then $f_t$ is Gaussian for all times $t$.
\item Assuming that the kernel is positive definite on the data (implying that the $Nn_L \times Nn_L$ Gram matrix $\tilde{K} = (K_{kk'}(x_i,x_j))_{ik,jk'}$ is invertible), as $t \rightarrow \infty$ limit, we get that
\begin{align*}
f_\infty = f^* + \Delta^0_f = f_0 - \sum_{i = 1}^{Nn_L} \Delta^i_f
\end{align*}
takes the form
\begin{align*}
f_{\infty,k}(x) = \kappa^\top_{x,k} \tilde{K}^{-1} y^* + (f_0(x) - \kappa^\top_{x,k} \tilde{K}^{-1} y_0)
\end{align*}
with the $Nn_L$-vectors $\kappa_{x,k}$, $y^*$, and $y_0$ given by
\begin{align*}
\kappa_{x,k} &= (K_{kk'}(x,x_i))_{i,k'} \\
y^* &= (f^*_k(x_i))_{i,k} \\
y_0 &= (f_{0,k}(x_i))_{i,k}.
\end{align*}
\item The expression for $f_\infty(x)$ might be confusing. Let us analyze the expression. We observe that
\begin{align*}
\sum_{i = 1}^{Nn_L} \Delta^i_f
\end{align*}
corresponds to the projection of $f^* - f_0$ onto the range of $\Pi$. This is because, by definition,
\begin{align*}
\sum_{i = 1}^{Nn_L} \Delta^i_f = (f^* - f_0) - \Delta^0_f,
\end{align*}
and $\Delta^0_f$ is the component of $f^* - f_0$ in the null space of $\Pi$. To verify the relation
\begin{align*}
\sum_{i = 1}^{Nn_L} \Delta^i_f = \kappa^\top_{x,k} \tilde{K}^{-1} (y^* - y_0),
\end{align*}
we thus need to check two facts.
\begin{itemize}
\item $\kappa^\top_{x,k} \tilde{K}^{-1} (y^* - y_0)$ is in the range of $\Pi$.
\item $\Pi(\kappa^\top_{x,k} \tilde{K}^{-1} (y^* - y_0)) = \Pi(f)$.
\end{itemize}
Recall that
\begin{align*}
\Pi(f)_k(x) = \frac{1}{N} \sum_{i = 1}^N \sum_{k' = 1}^{n_L} f_{k'}(x_i) K_{kk'}(x_i,x).
\end{align*}
The range of $\Pi$ is the collection of functions that can be expressed as a linear combination of $\{K_{kk'}(x_i,x)\}_{i,k'}$. We indeed observe that $\kappa^\top_{x,k} \tilde{K}^{-1} (y^* - y_0)$ is a linear combination of $\{K_{kk'}(x_i,x)\}_{i,k'}$. Moreover, it can easily be calculated that
\begin{align*}
\kappa^\top_{\textcolor{red}{x_i},k} \tilde{K}^{-1} (y^* - y_0) = (f^* - f_0)(x_i)
\end{align*}
and so the second point is true as well.
\item The first term, $\kappa^\top_{x,k} \tilde{K}^{-1} y^*$, has an important statistical interpretation. It is the maximum-a-posteriori (MAP) estimate given a Gaussian prior on functions $f_k \sim \NN(0,\Theta^\paran{L}_\infty)$ and the conditions $f_k(x_i) = f^*_k(x_i)$. For instance, see Equation (2.19) in \url{http://www.gaussianprocess.org/gpml/chapters/RW.pdf}. Equivalently, it is equal to the kernel ridge regression as the regularization goes to zero.
\item The second term $(f_0(x) - \kappa^\top_{x,k} \tilde{K}^{-1} y_0)$ is a centered Gaussian whose variance vanishes on the points of the dataset. Indeed, since $f_{0,k} \sim \NN(0,\Theta^\paran{L}_\infty)$,
\begin{align*}
\EE[f_0(x) - \kappa^\top_{x,k} \tilde{K}^{-1} y_0] = \EE[f_0(x)] - \kappa^\top_{x,k} \tilde{K}^{-1} \EE[y_0] = 0.
\end{align*}
Also,
\begin{align*}
f_0(x_i) - \kappa^\top_{x_i,k} \tilde{K}^{-1} y_0 = f_0(x_i) - f_0(x_i) = 0
\end{align*}
so the variance vanishes on the points of the dataset.
\item This shows that in the infinite-width limit, we are essentially fitting a Gaussian process to the dataset! Indeed, see Figure 2.
\end{itemize}

\appendix

\section{Appendix}

\begin{itemize}
\item We study the limit of the NTK as $n_1, \ldots, n_{L - 1} \rightarrow \infty$ sequentially. That is, we first take $n_1 \rightarrow \infty$, then $n_2 \rightarrow \infty$, and so on. This leads to much simpler proofs, but our results could in principle by strengthened to the more general setting when $\min(n_1, \ldots, n_{L - 1}) \rightarrow \infty$.
\item A natural choice of convergence to study the NTK is w.r.t. the operator norm on kernels
\begin{align*}
\|K\|_\op = \max_{\|f\|_\pin \leq 1} \|f\|_K = \max_{\|f\|_\pin \leq 1} \sqrt{\EE_{x,x' \sim \pin}[f(x)^\top K(x,x') f(x')]}.
\end{align*}
\item Define
\begin{align*}
\mathbf{f} \coloneqq (f(x_1), \ldots, f(x_N)) \in \RR^{Nn_L}, \qquad \mathbf{K} \coloneqq (K_{kk'}(x_i,x_j))_{k,k' < n_L, i,j < N} \in \RR^{Nn_L \times Nn_L}.
\end{align*}
We then have
\begin{align*}
\| f \|_\pin^2 = \inner{f}{f}_\pin = \EE_{x \sim \pin} [ \|f(x)\|^2_2 ] = \frac{1}{N} \sum_{i = 1}^N \|f(x_i)\|^2_2 = \frac{1}{N} \| (f(x_1), \ldots, f(x_N)) \|^2 = \frac{1}{N} \|\mathbf{f}\|^2_2.
\end{align*}
Also, note that
\begin{align*}
\EE_{x,x'}[f(x)^\top K(x,x') f(x')] &= \frac{1}{N^2} \sum_{i = 1}^N \sum_{j = 1}^N f(x_i)^\top K(x_i,x_j) f(x_j) = \frac{1}{N^2} \mathbf{f}^\top \mathbf{K} \mathbf{f}
\end{align*}
and so
\begin{align*}
\|K\|_\op = \frac{1}{N} \max_{\|\mathbf{f}\|_2 \leq \sqrt{N}} \sqrt{\mathbf{f}^\top \mathbf{K} \mathbf{f}} = \frac{1}{\sqrt{N}} \max_{\|\mathbf{f}\|_2 \leq 1} \sqrt{\mathbf{f}^\top \mathbf{K} \mathbf{f}} 
\end{align*}
Hence, $\|K\|_\op$ is equal to the (scaled) leading eigenvalue of the $Nn_L \times Nn_L$ Gram matrix $\mathbf{K}$.
\end{itemize}

\newpage

\subsection{Asymptotics at Initialization}

\textbf{Proposition 1.} \textit{For a network of depth $L$ at initialization, with a Lipschitz nonlinearity $\sigma$, and in the limit as $n_1, \ldots, n_{L - 1} \rightarrow \infty$ sequentially, the output functions $f_{\theta,k}$, for $k = 1, \ldots, n_L$, tend (in law) to i.i.d. centered Gaussian processes of covariance $\Sigma^\paran{L}$, where $\Sigma^\paran{L}$ is defined recursively by
\begin{align*}
\Sigma^\paran{1}(x,x') &= \frac{1}{n_0} x^\top x' + \beta^2, \\
\Sigma^\paran{L + 1}(x,x') &= \EE_{f \sim \NN(0,\Sigma^\paran{L})} [\sigma(f(x)) \sigma(f(x'))] + \beta^2
\end{align*}
where $\NN(0,\Sigma^\paran{L})$ denotes the centered Gaussian process with covariance function $\Sigma^\paran{L}$.}

\begin{proof}
We prove the result by induction. When $L = 1$, there are no hidden layers and $f_\theta$ is a random affine function of the form
\begin{align*}
f_\theta(x) = \frac{1}{\sqrt{n_0}} W^\paran{0} x + \beta b^\paran{0}.
\end{align*}
For any index $i$, we thus have
\begin{align*}
\EE[f_{\theta,i}(x)] = \EE \left[ \frac{1}{\sqrt{n_0}} \sum_{j = 1}^{n_0} W^\paran{0}_{i,j} x_j + \beta b^\paran{0}_i \right] = \frac{1}{\sqrt{n_0}} \sum_{j = 1}^{n_0} \EE[ W^\paran{0}_{i,j} ] x_j + \beta \EE[ b^\paran{0}_i ] = 0
\end{align*}
because the parameters are initialized from $\NN(0,1)$. It follows that
\begin{align*}
\Sigma^\paran{1}(x,x') &= \EE[ f_{\theta,i}(x) f_{\theta,i}(x') ] - \EE[f_{\theta,i}(x)] \EE[f_{\theta,i}(x')] \\
&= \EE[ f_{\theta,i}(x) f_{\theta,i}(x') ] \\
&= \frac{1}{n_0} \sum_{j = 1}^{n_0} \sum_{j' = 1}^{n_0} \EE[W^\paran{0}_{i,j} W^\paran{0}_{i,j'}]x_j x_{j'}' + \frac{\beta}{\sqrt{n_0}} \sum_{j = 1}^{n_0} \EE[ W^\paran{0}_{i,j} \beta^\paran{0}_i](x_j + x_j') + \beta^2 \EE[(b^\paran{0}_i)^2] \\
&= \frac{1}{n_0} x^\top x' + \beta^2
\end{align*}
which proves $f_{\theta,i} \sim \NN(0,\Sigma^\paran{1})$. This concludes the base case.

The key to the induction step is to consider an $(L + 1)$-network as the following composition. An $L$-network $\RR^{n_0} \rightarrow \RR^{n_L}$ mapping the input to the pre-activations $\tilde{\alpha}^\paran{L}_i$, followed by an elementwise application of the nonlinearity $\sigma$ and then a random affine map $\RR^{n_L} \rightarrow \RR^{n_{L + 1}}$. The induction hypothesis gives in the limit as sequentially $n_1, \ldots, n_{L - 1} \rightarrow \infty$, the preactivations $\tilde{\alpha}^\paran{L}_i$ tend to i.i.d. Gaussian processes with covariance $\Sigma^\paran{L}$. Then, conditioned on $\tilde{\alpha}^\paran{L}_i \sim \NN(0,\Sigma^\paran{L})$, the outputs
\begin{align*}
f_{\theta,i}(x) = \frac{1}{\sqrt{n_L}} \sum_{j = 1}^{n_L} W^\paran{L}_{i,j} \sigma(\tilde{\alpha}^\paran{L}_j(x;\theta)) + \beta b^\paran{L}_i
\end{align*}
are i.i.d. centered Gaussians with covariance
\begin{align*}
\tilde{\Sigma}^\paran{L + 1}(x,x') = \EE[f_{\theta,i}(x) f_{\theta,i}(x')] = \frac{1}{n_L} \sigma(\tilde{\alpha}^\paran{L}(x;\theta))^\top \sigma(\tilde{\alpha}^\paran{L}(x;\theta)) + \beta^2.
\end{align*}
In other words, conditioned on $\tilde{\alpha}^\paran{L}_i \sim \NN(0,\Sigma^\paran{L})$, $f_{\theta,i} \sim \NN(0,\tilde{\Sigma}^\paran{L + 1})$. The proof for this is identical to the proof for the base case. Specifically, repeat the proof for the base case with $\sigma(\tilde{\alpha}^\paran{L}(x;\theta))$ in place of $x$. By the law of large numbers, as $n_L \rightarrow \infty$, this covariance tends in probability to the expectation
\begin{align*}
\Sigma^\paran{L + 1}(x,x') &\coloneqq \lim_{n_L \rightarrow \infty} \tilde{\Sigma}^{L + 1}(x,x') \\
&= \lim_{n_L \rightarrow \infty} \frac{1}{n_L} \sigma(\tilde{\alpha}^\paran{L}(x;\theta))^\top \sigma(\tilde{\alpha}^\paran{L}(x;\theta)) + \beta^2 \\
&= \lim_{n_L \rightarrow \infty} \frac{1}{n_L} \sum_{i = 1}^{n_L} \sigma(\tilde{\alpha}^\paran{L}_i(x;\theta)) \sigma(\tilde{\alpha}^\paran{L}_i(x';\theta)) + \beta^2 \\
&= \EE_{f \sim \NN(0,\Sigma^\paran{L})}[\sigma(f(x))\sigma(f(x'))] + \beta^2
\end{align*}
because $\tilde{\alpha}^\paran{L}_i$ are i.i.d. samples from $\NN(0,\Sigma^\paran{L})$. In particular, the covariance is deterministic and hence independent of $\alpha^\paran{L}$. As a consequence, the conditioned and unconditioned distributions of $f_{\theta,i}$ are equal in the limit. They are i.i.d. centered Gaussian processes of covariance $\Sigma^\paran{L + 1}$. This concludes the induction step and so the proposition is proved.
\end{proof}

\textbf{Theorem 1.} \textit{For a network of depth $L$ at initialization, with a Lipschitz nonlinearity $\sigma$, and in the limit as the layers width $n_1, \ldots, n_{L - 1} \rightarrow \infty$ sequentially, the NTK $\Theta^\paran{L}$ converges in probability to a deterministic limiting kernel
\begin{align*}
\Theta^\paran{L} \rightarrow \Theta^\paran{L}_\infty \otimes Id_{n_L}.
\end{align*}
The scalar kernel $\Theta^\paran{L}_\infty : \RR^{n_0} \times \RR^{n_0} \rightarrow \RR$ is defined recursively by
\begin{align*}
\Theta^\paran{1}_\infty(x,x') &= \Sigma^\paran{1}(x,x') \\
\Theta^\paran{L + 1}_\infty(x,x') &= \Theta^\paran{L}(x,x') \dot{\Sigma}^\paran{L + 1}(x,x') + \Sigma^\paran{L + 1}(x,x'),
\end{align*}
where
\begin{align*}
\dot{\Sigma}^\paran{L + 1}(x,x') = \EE_{f \sim \NN(0,\Sigma^\paran{L})} [ \dot{\sigma}(f(x)) \dot{\sigma}(f(x'))],
\end{align*}
where $\dot{\sigma}$ denotes the derivative of $\sigma$.}

\newpage

\subsection{Asymptotics During Training}

Given a training direction $t \mapsto d_t \in \FF$, a neural network is trained in the following manner: the parameters $\theta_p$ are initialized as i.i.d. $\NN(0,1)$ and follow the differential equation
\begin{align*}
\partial_t \theta_p(t) = \inner{\partial_{\theta_p} F^\paran{L}(\theta(t))}{d_t}_\pin.
\end{align*}

\textbf{Theorem 2.} \textit{Assume that $\sigma$ is a Lipschitz twice differentiable nonlinearity function, with bounded second derivative. For any $T$ such that the integral $\int_0^T \|d_t\|_\pin \, dt$ stays stochastically bounded, as $n_1, \ldots, n_{L - 1} \rightarrow \infty$ sequentially, we have, uniformly for $t \in [0,T]$,
\begin{align*}
\Theta^\paran{L}(t) \rightarrow \Theta^\paran{L}_\infty \otimes Id_{n_L}.
\end{align*}
As a consequence, in this limit, the dynamics of $f_\theta$ is described by the differential equation
\begin{align*}
\partial_t f_{\theta(t)} = \Phi_{\Theta^\paran{L}_\infty \otimes Id_{n_L}} \left( \inner{d_t}{\cdot}_\pin \right).
\end{align*}}

\begin{proof}
Let $\tilde{\theta}$ be the parameters of the smaller network, and let $\theta_p \in \tilde{\theta}$. Then
\begin{align*}
\partial_{\theta_p} F^\paran{L + 1}(\theta) &= \partial_{\theta_p} \left( \frac{1}{\sqrt{n_L}} W^\paran{L} \sigma( F^\paran{L}(\tilde{\theta}) ) + \beta b^\paran{L} \right) \\
&= \frac{1}{\sqrt{n_L}} W^\paran{L} \dot{\sigma}( F^\paran{L}(\tilde{\theta}) ) \partial_{\theta_p} F^\paran{L}(\tilde{\theta})
\end{align*}
and so
\begin{align*}
\partial_t \theta_p(t) &= \inner{\partial_{\theta_p} F^\paran{L}(\theta(t))}{d_t}_\pin \\
&= \inner{\frac{1}{\sqrt{n_L}} W^\paran{L}(t) \dot{\sigma}( F^\paran{L}(\tilde{\theta}(t)) ) \partial_{\theta_p} F^\paran{L}(\tilde{\theta}(t))}{d_t}_\pin \\
&= \inner{\partial_{\theta_p} F^\paran{L}(\tilde{\theta}(t))}{\dot{\sigma}( F^\paran{L}(\tilde{\theta}(t)) ) \left( \frac{1}{\sqrt{n_L}} W^\paran{L}(t) \right)^\top d_t}
\end{align*}
which implies that the smaller network follows the training direction
\begin{align*}
d_t' = \dot{\sigma}( F^\paran{L}(\tilde{\theta}(t)) ) \left( \frac{1}{\sqrt{n_L}} W^\paran{L}(t) \right)^\top d_t.
\end{align*}
Since $\sigma$ is a $c$-Lipschitz function, $|\dot{\sigma}| \leq c$ and so
\begin{align*}
\|d_t'\|_\pin &\leq |\dot{\sigma}( F^\paran{L}(\tilde{\theta}(t)) )| \left\| \frac{1}{\sqrt{n_L}} W^\paran{L}(t) \right\|_{op} \|d_t\|_\pin \\
&\leq c \left\| \frac{1}{\sqrt{n_L}} W^\paran{L}(t) \right\|_{op} \|d_t\|_\pin.
\end{align*}

From the law of large numbers,
\begin{align*}
\left\| \frac{1}{\sqrt{n_L}} W^\paran{L}_i(0) \right\|_2^2 &= \frac{1}{n_L} \sum_{j = 1}^{n_L} W_{ij}^2(0) \rightarrow \EE[W_{ij}^2(0)] = 1
\end{align*}
since $W_{ij}(0)$ are i.i.d. samples from $\NN(0,1)$. Hence, $\|\frac{1}{\sqrt{n_L}} W^\paran{L}(0)\|_{op}$ is bounded.

Observe that by the triangle inequality,
\begin{align*}
\partial_t \|f(t)\| = \lim_{h \rightarrow 0} \frac{\|f(t + h)\| - \|f(t)\|}{h} \leq \lim_{h \rightarrow 0} \frac{\|f(t + h) - f(t)\|}{h} = \| \partial_t f(t)\|
\end{align*}
and so $\partial_t \|\cdot\| \leq \| \partial_t \cdot \|$. It follows that
\begin{align*}
\partial_t \left\| W_i^\paran{L}(t) - W_i^\paran{L}(0) \right\|_2 &\leq \left\| \partial_t \left( W_i^\paran{L}(t) - W_i^\paran{L}(0) \right) \right\|_2 \\
&= \left\| \partial_t W_i^\paran{L}(t) \right\|_2 \\
&\leq \frac{1}{\sqrt{n_L}} \| \alpha_i^\paran{L}(t) \|_\pin \|d_t\|_\pin.
\end{align*}

\begin{align*}
\partial_t \left( c \left\| \tilde{\alpha}^\paran{L}_i(t) - \tilde{\alpha}^\paran{L}_i(0) \right\|_\pin + \left\| W^\paran{L}_i(t) - W^\paran{L}_i(0) \right\|_2 \right) = \partial_t( A(t) - A(0) ) = \partial_t A(t) = O\left(\frac{1}{\sqrt{n_L}}\right).
\end{align*}

\begin{align*}
\partial_{W^\paran{L}_{ij}} f_{\theta(t),j'}(x) \otimes \partial_{W^\paran{L}_{ij}} f_{\theta(t),j''}(x') = \frac{1}{n_L} \alpha^\paran{L}_i(x;\theta(t))^2 \delta_{jj'} \delta_{jj''}
\end{align*}
and so
\begin{align*}
\partial_t \left( \partial_{W^\paran{L}_{ij}} f_{\theta(t),j'}(x) \otimes \partial_{W^\paran{L}_{ij}} f_{\theta(t),j''}(x') \right) = \frac{2}{n_L} \partial_t \alpha^\paran{L}_i(x;\theta(t)) \delta_{jj'} \delta_{jj''}
\end{align*}
and since $|\partial_t \alpha^\paran{L}_i| = O(\frac{1}{\sqrt{n_L}})$, we see that the summands vary at the rate $n_L^{-3/2}$. Since the dimension of $W^\paran{L}$ is $n_L \times n_{L + 1}$ (recall that $n_{L + 1}$ is fixed), the sum induces a variation of the NTK of rate $\frac{1}{\sqrt{n_L}}$.
\end{proof}

\newpage

\subsection{A Priori Control During Training}

\subsection{Positive-Definiteness of $\Theta^\paran{L}_\infty$}




























\end{document}